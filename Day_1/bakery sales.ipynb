{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"zcHCgKrdpzep","executionInfo":{"status":"ok","timestamp":1762369928423,"user_tz":-60,"elapsed":4337,"user":{"displayName":"K U","userId":"00248825267147556825"}}},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","from google.colab import drive"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"XSj1vO1Vbo8b","executionInfo":{"status":"error","timestamp":1762370050095,"user_tz":-60,"elapsed":121670,"user":{"displayName":"K U","userId":"00248825267147556825"}},"outputId":"6e13ca54-d866-4eb9-9ff3-2492d6e26cd3"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"mount failed","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1237719191.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'google.colab'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ… PÅ™ipojen Google Drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}],"source":["import sys\n","import pandas as pd\n","import seaborn as sns\n","\n","# OvÄ›Å™Ã­, zda bÄ›Å¾Ã­Å¡ v Colabu\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    print(\"âœ… PÅ™ipojen Google Drive\")\n","else:\n","    print(\"ðŸ’» BÄ›Å¾Ã­Å¡ lokÃ¡lnÄ› (Jupyter), Drive nenÃ­ potÅ™eba\")"]},{"cell_type":"markdown","metadata":{"id":"amHNr7dBcoo0"},"source":["## Sales at The Bread Basket bakery"]},{"cell_type":"markdown","metadata":{"id":"yu_kx6yurCMx"},"source":["\n","\n","In this notebook we will be visualizing the 2016-2017 sales data at *The Bread Basket* bakery in Edinburgh.\n","\n","The data comes from the set published at [kaggle](https://www.kaggle.com/akashdeepkuila/bakery) with the *CC0* license.\n","\n","Let's get familiar with the content of the notebook and follow the instructions to prepare the data that we are going to need during classes.\n","\n","**Note!** When you come back to the document later, remember to re-run the code cells."]},{"cell_type":"markdown","metadata":{"id":"GDtYniklctWI"},"source":["### Dataset contents"]},{"cell_type":"markdown","metadata":{"id":"QXCSzpZacxOt"},"source":["In the **bakery_sales.csv** file imported below there are 20 507 items assigned to 9 684 client transactions with the information on:\n","\n","\n","*   **TransactionNo** - transaction number\n","*   **Items** - purchased items\n","*   **DateTime** - time of transaction\n","*   **Daypart** - time of the day\n","*   **DayType** - weekday or weekend."]},{"cell_type":"markdown","metadata":{"id":"PuUlipO5tPgw"},"source":["The necessary data is provided with the document: the code below imports the files we need."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HtFv230NtYQ5"},"outputs":[],"source":["drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"JoW4s4OstlNS"},"source":["Data is imported to a pandas DataFrame which lets us work on visualizing them in an efficient way."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SlF-tcvntjif"},"outputs":[],"source":["bakery_data = pd.read_csv('/content/drive/My Drive/Vis/Bakery Data/bakery_sales.csv')\n","bakery_data"]},{"cell_type":"markdown","metadata":{"id":"6uBGMv_UdAdd"},"source":["## Notebook preparation"]},{"cell_type":"markdown","metadata":{"id":"_bjmNcUudBv7"},"source":["We want to start by making sure that the data has been correctly identified and make necessary conversions.\n","\n","Based on data overview, we expect the first column to contain consecutive integers; the second one: names of sold products; third: the data identified as time-based; and the last two columns should have text-based information."]},{"cell_type":"markdown","metadata":{"id":"7SwogOhlf8ED"},"source":["### Checking data types"]},{"cell_type":"markdown","metadata":{"id":"VvhK3R-VkUd5"},"source":["Below, we need to run the listed instructions to get the DataFrames used during classes."]},{"cell_type":"markdown","metadata":{"id":"6_tetvvaJnrH"},"source":["First let's check how the data was identified on import."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nKkIMZZgBSR"},"outputs":[],"source":["bakery_data.dtypes"]},{"cell_type":"markdown","metadata":{"id":"ROte86Mlke19"},"source":["Let's make sure the data has records with missing information for any of the columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vC3D6-GMjlM1"},"outputs":[],"source":["\"complete records: \" + str(len(bakery_data.dropna(how=\"any\"))) + \"; total records: \" + str(len(bakery_data))"]},{"cell_type":"markdown","metadata":{"id":"U7ZVMbRUceZj"},"source":["Let's also take a look at what data is really hidden under the **object** type for each of the columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9n54vfg2gQgn"},"outputs":[],"source":["for column in bakery_data.columns:\n","  check_types = bakery_data[column].apply(lambda x: type(x))\n","  print(check_types.value_counts())"]},{"cell_type":"markdown","metadata":{"id":"800icOcr_4Fn"},"source":["#### Date conversion"]},{"cell_type":"markdown","metadata":{"id":"BqHvg4O9ktXb"},"source":["In the case of transaction time it is by default identified as a *string*.\n","\n","Let's change the **DateTime** column data to *timestamp*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHZLsw6LlAZr"},"outputs":[],"source":["bakery_data[\"DateTime\"] = pd.to_datetime(bakery_data[\"DateTime\"])"]},{"cell_type":"markdown","metadata":{"id":"flVAzownJ3Tc"},"source":["We'll add a new column with translation date, callled **Date** and validate the conversion."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qu7S9J6xhonQ"},"outputs":[],"source":["bakery_data[\"Date\"] = bakery_data[\"DateTime\"].dt.date"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoHr-RjR12zw"},"outputs":[],"source":["bakery_data[\"Date\"].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"dQDBmrzVweYy"},"source":["Because we are not going to use the information about time, in **bakery_data** we can leave just the column with the date."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIEOHUZrwwNm"},"outputs":[],"source":["bakery_data = bakery_data[[\"TransactionNo\", \"Items\", \"Date\", \"Daypart\", \"DayType\"]]\n","bakery_data"]},{"cell_type":"markdown","metadata":{"id":"1giWuV7hknZ4"},"source":["#### Category assignment based on the number of sold products."]},{"cell_type":"markdown","metadata":{"id":"oXRX2shkliaa"},"source":["Let's take a closer look at the contents of the **Items** category."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJlBaQQTlgsn"},"outputs":[],"source":["bakery_data[\"Items\"].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"VWXfpO6DlngD"},"source":["We see that in the period we analyze many products were sold with a varied frequency.\n","\n","We'll add categorization by adding the **Item Categories** column that will enable us to highlight top 5 products and assign the \"Other\" category to the remaining ones."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4zs320Q4kskY"},"outputs":[],"source":["product_categories = list(bakery_data[\"Items\"].value_counts().index)[0:5]\n","product_categories.append(\"Other\")\n","product_categories\n","bakery_data[\"Item Categories\"] = pd.Series(pd.Categorical(bakery_data[\"Items\"], categories=product_categories)).fillna(\"Other\")\n","bakery_data"]},{"cell_type":"markdown","metadata":{"id":"2LWL0kTCpWcG"},"source":["#### Converting times of day to categories"]},{"cell_type":"markdown","metadata":{"id":"TNd8veZAKlMR"},"source":["Let's take a closer look at the contents of the **Daypart** category."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QhKOrhKpa73"},"outputs":[],"source":["bakery_data[\"Daypart\"].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"b2JnZ-XEKqOl"},"source":["In the case of this column the list of categories is a short one: we only want the order of the times of day in the visualization to be a natural one.\n","\n","We'll define a new **Day Part** column, set the correct category order and use it to replace the current **Daypart** column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDc76LOmpivJ"},"outputs":[],"source":["bakery_data[\"Day Part\"] = pd.Series(pd.Categorical(bakery_data[\"Daypart\"], categories=[\"Morning\", \"Afternoon\", \"Evening\", \"Night\"]))\n","bakery_data = bakery_data[[\"TransactionNo\", \"Items\", \"Date\", \"Day Part\", \"DayType\", \"Item Categories\"]]\n","bakery_data"]},{"cell_type":"markdown","metadata":{"id":"tWcawmFDqNXH"},"source":["#### Converting day types to categories"]},{"cell_type":"markdown","metadata":{"id":"JBuzp8rWLmNR"},"source":["Let's take a closer look at the **Day Type** column contents."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7tZj9cNqT0P"},"outputs":[],"source":["bakery_data[\"DayType\"].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"Gbp4kNDSLv4f"},"source":["Similarly to the time of day, the list of categories is short. We'll prepare a new **Day Type** column just like before and remove the unnecessary column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZSbhQ2uq9fq"},"outputs":[],"source":["bakery_data[\"Day Type\"] = pd.Series(pd.Categorical(bakery_data[\"DayType\"], categories=[\"Weekday\", \"Weekend\"]))\n","bakery_data = bakery_data[[\"TransactionNo\", \"Items\", \"Date\", \"Day Part\",\"Day Type\", \"Item Categories\"]]\n","bakery_data"]},{"cell_type":"markdown","metadata":{"id":"LUJu0fjmzpHl"},"source":["### Creating dataframes used in the visualization"]},{"cell_type":"markdown","metadata":{"id":"nKeJjrMJzuZG"},"source":["Besides the **bakery_data** set, for class we are going to need several other points of view to base our visualization on."]},{"cell_type":"markdown","metadata":{"id":"vBe3aL2jz82N"},"source":["#### Daily statistics"]},{"cell_type":"markdown","metadata":{"id":"A5XaF-_z-dAX"},"source":["Below, we are calculating how many products, and in how many transactions, were purchased daily, divided by type of day."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5LC4aCd50Fct"},"outputs":[],"source":["items_daily = bakery_data[[\"Date\",\"Day Type\", \"Items\"]].groupby([\"Date\", \"Day Type\"]).count()\n","transactions_daily = bakery_data[[\"Date\",\"Day Type\", \"TransactionNo\"]].groupby([\"Date\", \"Day Type\"]).nunique()\n","daytype_statistics_daily = pd.merge(items_daily, transactions_daily, on=[\"Date\", \"Day Type\"])\n","daytype_statistics_daily"]},{"cell_type":"markdown","metadata":{"id":"BeJUyKpE-q19"},"source":["Below, we are calculating how many products, and in how many transactions, were purchased daily, divided by time of day."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4cGsRBa9_QL"},"outputs":[],"source":["items_daily = bakery_data[[\"Date\",\"Day Part\", \"Items\"]].groupby([\"Date\", \"Day Part\"]).count()\n","transactions_daily = bakery_data[[\"Date\",\"Day Part\", \"TransactionNo\"]].groupby([\"Date\", \"Day Part\"]).nunique()\n","daypart_statistics_daily = pd.merge(items_daily, transactions_daily, on=[\"Date\", \"Day Part\"])\n","daypart_statistics_daily"]},{"cell_type":"markdown","metadata":{"id":"eWs2Miyj-zdD"},"source":["#### Category statistics"]},{"cell_type":"markdown","metadata":{"id":"FWt8QxEtMOTq"},"source":["Finally, we also return the number of products purchased via transactions with part and type of the day information."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOlmnEj7-34M"},"outputs":[],"source":["items_count = bakery_data[[\"TransactionNo\", \"Items\"]].groupby([\"TransactionNo\"]).count()\n","transactions_data = pd.merge(pd.DataFrame(bakery_data[[\"TransactionNo\", \"Day Type\", \"Day Part\"]].drop_duplicates()), items_count, on=\"TransactionNo\")\n","transactions_data"]},{"cell_type":"markdown","metadata":{"id":"bKC2RZl71xif"},"source":["## Exercises"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"}},"nbformat":4,"nbformat_minor":0}
